{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yx1Eos-ypL40",
        "outputId": "51b0f363-3af3-4493-bd1f-2e694fbfa9df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/cuda:\n",
            "bin\t\t   EULA.txt  libnvvp\t       nvvm-prev  src\n",
            "compat\t\t   extras    nsightee_plugins  README\t  targets\n",
            "compute-sanitizer  include   nvml\t       samples\t  tools\n",
            "DOCS\t\t   lib64     nvvm\t       share\t  version.json\n",
            "\n",
            "/usr/local/cuda-11:\n",
            "bin\t\t   EULA.txt  libnvvp\t       nvvm-prev  src\n",
            "compat\t\t   extras    nsightee_plugins  README\t  targets\n",
            "compute-sanitizer  include   nvml\t       samples\t  tools\n",
            "DOCS\t\t   lib64     nvvm\t       share\t  version.json\n",
            "\n",
            "/usr/local/cuda-11.2:\n",
            "bin\t\t   EULA.txt  libnvvp\t       nvvm-prev  src\n",
            "compat\t\t   extras    nsightee_plugins  README\t  targets\n",
            "compute-sanitizer  include   nvml\t       samples\t  tools\n",
            "DOCS\t\t   lib64     nvvm\t       share\t  version.json\n"
          ]
        }
      ],
      "source": [
        "!ls /usr/local/cuda*"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7J5GR1ApVxu",
        "outputId": "2d723def-a08b-4110-bb0e-3cdd4a2e9138"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Dec 16 17:10:23 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   56C    P0    28W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile matrix.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <assert.h>\n",
        "#include <sys/time.h>\n",
        "\n",
        "#define BLOCK_SIZE 16\n",
        "\n",
        "double cpuSecond() {\n",
        "   struct timeval tp;\n",
        "   gettimeofday(&tp,NULL);\n",
        "   return ((double)tp.tv_sec + (double)tp.tv_usec*1.e-6);\n",
        "}\n",
        "\n",
        "\n",
        "__global__ void gpu_matrix_mult(int *a,int *b, int *c, int m, int n, int k)\n",
        "{ \n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y; \n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int sum = 0;\n",
        "    if( col < k && row < m) \n",
        "    {\n",
        "        for(int i = 0; i < n; i++) \n",
        "        {\n",
        "            sum += a[row * n + i] * b[i * k + col];\n",
        "        }\n",
        "        c[row * k + col] = sum;\n",
        "    }\n",
        "} \n",
        "\n",
        "\n",
        "__global__ void gpu_square_matrix_mult(int *d_a, int *d_b, int *d_result, int n) \n",
        "{\n",
        "    __shared__ int tile_a[BLOCK_SIZE][BLOCK_SIZE];\n",
        "    __shared__ int tile_b[BLOCK_SIZE][BLOCK_SIZE];\n",
        "\n",
        "    int row = blockIdx.y * BLOCK_SIZE + threadIdx.y;\n",
        "    int col = blockIdx.x * BLOCK_SIZE + threadIdx.x;\n",
        "    int tmp = 0;\n",
        "    int idx;\n",
        "\n",
        "    for (int sub = 0; sub < gridDim.x; ++sub) \n",
        "    {\n",
        "        idx = row * n + sub * BLOCK_SIZE + threadIdx.x;\n",
        "        if(idx >= n*n)\n",
        "        {\n",
        "            // n may not divisible by BLOCK_SIZE\n",
        "            tile_a[threadIdx.y][threadIdx.x] = 0;\n",
        "        }\n",
        "        else\n",
        "        {\n",
        "            tile_a[threadIdx.y][threadIdx.x] = d_a[idx];\n",
        "        }\n",
        "\n",
        "        idx = (sub * BLOCK_SIZE + threadIdx.y) * n + col;\n",
        "        if(idx >= n*n)\n",
        "        {\n",
        "            tile_b[threadIdx.y][threadIdx.x] = 0;\n",
        "        }  \n",
        "        else\n",
        "        {\n",
        "            tile_b[threadIdx.y][threadIdx.x] = d_b[idx];\n",
        "        }\n",
        "        __syncthreads();\n",
        "\n",
        "        for (int k = 0; k < BLOCK_SIZE; ++k) \n",
        "        {\n",
        "            tmp += tile_a[threadIdx.y][k] * tile_b[k][threadIdx.x];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "    if(row < n && col < n)\n",
        "    {\n",
        "        d_result[row * n + col] = tmp;\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "__global__ void gpu_matrix_transpose(int* mat_in, int* mat_out, unsigned int rows, unsigned int cols) \n",
        "{\n",
        "    unsigned int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    unsigned int idy = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\n",
        "    if (idx < cols && idy < rows) \n",
        "    {\n",
        "        unsigned int pos = idy * cols + idx;\n",
        "        unsigned int trans_pos = idx * rows + idy;\n",
        "        mat_out[trans_pos] = mat_in[pos];\n",
        "    }\n",
        "}\n",
        "\n",
        "void cpu_matrix_mult(int *h_a, int *h_b, int *h_result, int m, int n, int k) {\n",
        "    for (int i = 0; i < m; ++i) \n",
        "    {\n",
        "        for (int j = 0; j < k; ++j) \n",
        "        {\n",
        "            int tmp = 0.0;\n",
        "            for (int h = 0; h < n; ++h) \n",
        "            {\n",
        "                tmp += h_a[i * n + h] * h_b[h * k + j];\n",
        "            }\n",
        "            h_result[i * k + j] = tmp;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "int main(int argc, char const *argv[])\n",
        "{\n",
        "    int m, n, k;\n",
        "    /* Fixed seed for illustration */\n",
        "    srand(3333);\n",
        "    printf(\"please type in m n and k\\n\");\n",
        "    scanf(\"%d %d %d\", &m, &n, &k);\n",
        "\n",
        "    // allocate memory in host RAM, h_cc is used to store CPU result\n",
        "    int *h_a, *h_b, *h_c, *h_cc;\n",
        "    cudaMallocHost((void **) &h_a, sizeof(int)*m*n);\n",
        "    cudaMallocHost((void **) &h_b, sizeof(int)*n*k);\n",
        "    cudaMallocHost((void **) &h_c, sizeof(int)*m*k);\n",
        "    cudaMallocHost((void **) &h_cc, sizeof(int)*m*k);\n",
        "\n",
        "    // random initialize matrix A\n",
        "    for (int i = 0; i < m; ++i) {\n",
        "        for (int j = 0; j < n; ++j) {\n",
        "            h_a[i * n + j] = rand() % 1024;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // random initialize matrix B\n",
        "    for (int i = 0; i < n; ++i) {\n",
        "        for (int j = 0; j < k; ++j) {\n",
        "            h_b[i * k + j] = rand() % 1024;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    float gpu_elapsed_time_ms, cpu_elapsed_time_ms;\n",
        "\n",
        "    // some events to count the execution time\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    // start to count execution time of GPU version\n",
        "    cudaEventRecord(start, 0);\n",
        "    // Allocate memory space on the device \n",
        "    int *d_a, *d_b, *d_c;\n",
        "    cudaMalloc((void **) &d_a, sizeof(int)*m*n);\n",
        "    cudaMalloc((void **) &d_b, sizeof(int)*n*k);\n",
        "    cudaMalloc((void **) &d_c, sizeof(int)*m*k);\n",
        "\n",
        "    // copy matrix A and B from host to device memory\n",
        "    // TIME THIS\n",
        "    double hTdTime = cpuSecond();\n",
        "    cudaMemcpy(d_a, h_a, sizeof(int)*m*n, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_b, h_b, sizeof(int)*n*k, cudaMemcpyHostToDevice);\n",
        "    cudaDeviceSynchronize();\n",
        "    double hTdTimeElaplsed = cpuSecond() - hTdTime;\n",
        "\n",
        "    unsigned int grid_rows = (m + BLOCK_SIZE - 1) / BLOCK_SIZE;\n",
        "    unsigned int grid_cols = (k + BLOCK_SIZE - 1) / BLOCK_SIZE;\n",
        "    dim3 dimGrid(grid_cols, grid_rows);\n",
        "    dim3 dimBlock(BLOCK_SIZE, BLOCK_SIZE);\n",
        "   \n",
        "    // Launch kernel \n",
        "    //TIME THIS\n",
        "    double kernelTime = cpuSecond();\n",
        "    if(m == n && n == k)\n",
        "    {\n",
        "        gpu_square_matrix_mult<<<dimGrid, dimBlock>>>(d_a, d_b, d_c, n);    \n",
        "    }\n",
        "    else\n",
        "    {\n",
        "        gpu_matrix_mult<<<dimGrid, dimBlock>>>(d_a, d_b, d_c, m, n, k);    \n",
        "    }\n",
        "    cudaDeviceSynchronize();\n",
        "    double kernelTimeElaplsed = cpuSecond() - kernelTime;\n",
        "\n",
        "    // Transefr results from device to host \n",
        "    //TIME THIS\n",
        "    double dThTime = cpuSecond();\n",
        "    cudaMemcpy(h_c, d_c, sizeof(int)*m*k, cudaMemcpyDeviceToHost);\n",
        "    cudaDeviceSynchronize();\n",
        "    double dThTimeElaplsed = cpuSecond() - dThTime;\n",
        "\n",
        "    cudaThreadSynchronize();\n",
        "    // time counting terminate\n",
        "    cudaEventRecord(stop, 0);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    // compute time elapse on GPU computing\n",
        "    cudaEventElapsedTime(&gpu_elapsed_time_ms, start, stop);\n",
        "    printf(\"Time elapsed on matrix multiplication of %dx%d . %dx%d on GPU: %f ms.\\n\\n\", m, n, n, k, gpu_elapsed_time_ms);\n",
        "\n",
        "    // start the CPU version\n",
        "    cudaEventRecord(start, 0);\n",
        "\n",
        "    cpu_matrix_mult(h_a, h_b, h_cc, m, n, k);\n",
        "\n",
        "    cudaEventRecord(stop, 0);\n",
        "    cudaEventSynchronize(stop);\n",
        "    cudaEventElapsedTime(&cpu_elapsed_time_ms, start, stop);\n",
        "    printf(\"Time elapsed on matrix multiplication of %dx%d . %dx%d on CPU: %f ms.\\n\\n\", m, n, n, k, cpu_elapsed_time_ms);\n",
        "\n",
        "    // validate results computed by GPU\n",
        "    int all_ok = 1;\n",
        "    for (int i = 0; i < m; ++i)\n",
        "    {\n",
        "        for (int j = 0; j < k; ++j)\n",
        "        {\n",
        "            //printf(\"[%d][%d]:%d == [%d][%d]:%d, \", i, j, h_cc[i*k + j], i, j, h_c[i*k + j]);\n",
        "            if(h_cc[i*k + j] != h_c[i*k + j])\n",
        "            {\n",
        "                all_ok = 0;\n",
        "            }\n",
        "        }\n",
        "        //printf(\"\\n\");\n",
        "    }\n",
        "\n",
        "    // roughly compute speedup\n",
        "    if(all_ok)\n",
        "    {\n",
        "        printf(\"all results are correct!!!, speedup = %f\\n\", cpu_elapsed_time_ms / gpu_elapsed_time_ms);\n",
        "    }\n",
        "    else\n",
        "    {\n",
        "        printf(\"incorrect results\\n\");\n",
        "    }\n",
        "\n",
        "    printf(\"Host to Device: %f\\n\", hTdTimeElaplsed);\n",
        "    printf(\"Kernel: %f\\n\", kernelTimeElaplsed);\n",
        "    printf(\"Device to Host: %f\\n\", dThTimeElaplsed);\n",
        "\n",
        "    // free memory\n",
        "    cudaFree(d_a);\n",
        "    cudaFree(d_b);\n",
        "    cudaFree(d_c);\n",
        "    cudaFreeHost(h_a);\n",
        "    cudaFreeHost(h_b);\n",
        "    cudaFreeHost(h_c);\n",
        "    cudaFreeHost(h_cc);\n",
        "    return 0;\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAwWEDH_qYCz",
        "outputId": "daf66bb6-91cf-4ba5-de24-0824b3b97441"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting matrix.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc matrix.cu -o matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S35iQCcXq2GO",
        "outputId": "69d1099f-2134-4869-a12d-8515c0238059"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[Kmatrix.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kint main(int, const char**)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kmatrix.cu:258:23:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KcudaError_t cudaThreadSynchronize()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     cudaThreadSynchron\u001b[01;35m\u001b[Ki\u001b[m\u001b[Kze();\n",
            "                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda_runtime_api.h:1011:46:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " extern __CUDA_DEPRECATED __host__ cudaError_t\u001b[01;36m\u001b[K CUDARTAPI cudaThread\u001b[m\u001b[KSynchronize(void);\n",
            "                                              \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7siBWi1rYs-",
        "outputId": "0ef2cd44-5366-433a-8b5e-58108981e9ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "please type in m n and k\n",
            "1050 1060 1050\n",
            "Time elapsed on matrix multiplication of 1050x1060 . 1060x1050 on GPU: 11.244896 ms.\n",
            "\n",
            "Time elapsed on matrix multiplication of 1050x1060 . 1060x1050 on CPU: 5862.668457 ms.\n",
            "\n",
            "all results are correct!!!, speedup = 521.362610\n",
            "Host to Device: 0.000759\n",
            "Kernel: 0.009699\n",
            "Device to Host: 0.000349\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/cuda-11/bin/nv-nsight-cu-cli --metrics \tsm__warps_active.avg.pct_of_peak_sustained_active matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8slz68wIwx73",
        "outputId": "1de58570-ed9e-4abf-fa57-c7487d5637d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "please type in m n and k\n",
            "511 1023 4094\n",
            "==PROF== Connected to process 513 (/content/matrix)\n",
            "==PROF== Profiling \"gpu_matrix_mult\" - 1: 0%....50%....100% - 1 pass\n",
            "Time elapsed on matrix multiplication of 511x1023 . 1023x4094 on GPU: 278.681122 ms.\n",
            "\n",
            "Time elapsed on matrix multiplication of 511x1023 . 1023x4094 on CPU: 14554.444336 ms.\n",
            "\n",
            "all results are correct!!!, speedup = 52.226158\n",
            "Host to Device: 0.001603\n",
            "Kernel: 0.001603\n",
            "Device to Host: 0.000801\n",
            "==PROF== Disconnected from process 513\n",
            "[513] matrix@127.0.0.1\n",
            "  gpu_matrix_mult(int*, int*, int*, int, int, int), 2022-Dec-16 17:51:37, Context 1, Stream 7\n",
            "    Section: Command line profiler metrics\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    sm__warps_active.avg.pct_of_peak_sustained_active                                    %                          98.38\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Size = 1024 1024 1024\n",
        "# Host to Device: 0.000714\n",
        "# Kernel: 0.005817\n",
        "# Device to Host: 0.000330\n",
        "\n",
        "# Size = 1050 1050 1050\n",
        "# Host to Device: 0.000754\n",
        "# Kernel: 0.006478\n",
        "# Device to Host: 0.000350\n",
        "\n",
        "# Size = 1050 1060 1050\n",
        "# Host to Device: 0.001603\n",
        "# Kernel: 0.001603\n",
        "# Device to Host: 0.000801\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Data for the bar chart\n",
        "x = ['A', 'B', 'C']\n",
        "y1 = [0.000714,0.000754,0.001603]\n",
        "y2 = [0.005817,0.006478,0.001603]\n",
        "y3 = [0.000330,0.000350, 0.000801]\n",
        "\n",
        "# Create the figure and the axis\n",
        "\n",
        "\n",
        "# Add the data to the plot\n",
        "plt.bar(x, y1, label=\"Host to Device\")\n",
        "plt.bar(x, y2, label=\"kernel\")\n",
        "plt.bar(x, y3, label=\"Device to Host\")\n",
        "\n",
        "# Add labels and title\n",
        "# plt.set_xlabel(\"Vector size\")\n",
        "# plt.set_ylabel(\"Time taken\")\n",
        "# plt.set_title(\"Time taken based on Vector size\")\n",
        "plt.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "VN3COPeDDJxn",
        "outputId": "b556bdde-a387-4441-e9dc-9610441b6bf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}