{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIRohPehNgfO",
        "outputId": "fb30a4a1-5d30-4eb1-e752-e03dc1bc8141"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/cuda:\n",
            "bin\t\t   EULA.txt  libnvvp\t       nvvm-prev  src\n",
            "compat\t\t   extras    nsightee_plugins  README\t  targets\n",
            "compute-sanitizer  include   nvml\t       samples\t  tools\n",
            "DOCS\t\t   lib64     nvvm\t       share\t  version.json\n",
            "\n",
            "/usr/local/cuda-11:\n",
            "bin\t\t   EULA.txt  libnvvp\t       nvvm-prev  src\n",
            "compat\t\t   extras    nsightee_plugins  README\t  targets\n",
            "compute-sanitizer  include   nvml\t       samples\t  tools\n",
            "DOCS\t\t   lib64     nvvm\t       share\t  version.json\n",
            "\n",
            "/usr/local/cuda-11.2:\n",
            "bin\t\t   EULA.txt  libnvvp\t       nvvm-prev  src\n",
            "compat\t\t   extras    nsightee_plugins  README\t  targets\n",
            "compute-sanitizer  include   nvml\t       samples\t  tools\n",
            "DOCS\t\t   lib64     nvvm\t       share\t  version.json\n"
          ]
        }
      ],
      "source": [
        "!ls /usr/local/cuda*"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mx9K4mNcNnnJ",
        "outputId": "dcd47976-e194-4f4a-a559-28e6e8957332"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Dec 16 22:09:58 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P0    27W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile histo.cu\n",
        "\n",
        "#include<math.h>\n",
        "#include <stdio.h>\n",
        "#include<time.h>\n",
        "#include <iostream>\n",
        "#include <sys/time.h>\n",
        "#include <stdlib.h>\n",
        "#include \"cuda_runtime.h\"\n",
        "#include \"device_launch_parameters.h\"\n",
        "#include \"device_functions.h\"\n",
        "using namespace std;\n",
        "\n",
        "#define BLOCK_SIZE 512\n",
        "#define NUM_BINS 4096\n",
        "#define MAX_VAL 127\n",
        "#define PRIVATE 4096\n",
        "\n",
        "double cpuSecond() {\n",
        "   struct timeval tp;\n",
        "   gettimeofday(&tp,NULL);\n",
        "   return ((double)tp.tv_sec + (double)tp.tv_usec*1.e-6);\n",
        "}\n",
        "\n",
        "// histogramCPU computes the histogram of an input array on the CPU\n",
        "void histogramCPU(unsigned int* input, unsigned int* bins, unsigned int numElems) {\n",
        "    for (int i=0; i<numElems; i++) {\n",
        "        if (bins[input[i]] < MAX_VAL) {\n",
        "            bins[input[i]]++;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "// histogramGPU computes the histogram of an input array on the GPU\n",
        "__global__ void histogramGPU(unsigned int* input, unsigned int* bins, unsigned int numElems) {\n",
        "    int tx = threadIdx.x; int bx = blockIdx.x;\n",
        "\n",
        "    // compute global thread coordinates\n",
        "    int i = (bx * blockDim.x) + tx;\n",
        "\n",
        "    // create a private histogram copy for each thread block\n",
        "    __shared__ unsigned int hist[PRIVATE];\n",
        "\n",
        "    // each thread must initialize more than 1 location\n",
        "    if (PRIVATE > BLOCK_SIZE) {\n",
        "        for (int j=tx; j<PRIVATE; j+=BLOCK_SIZE) {\n",
        "            if (j < PRIVATE) {\n",
        "                hist[j] = 0;\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    // use the first `PRIVATE` threads of each block to init\n",
        "    else {\n",
        "        if (tx < PRIVATE) {\n",
        "            hist[tx] = 0;\n",
        "        }\n",
        "    }\n",
        "    // wait for all threads in the block to finish\n",
        "    __syncthreads();\n",
        "\n",
        "    // update private histogram\n",
        "    if (i < numElems) {\n",
        "        atomicAdd(&(hist[input[i]]), 1);\n",
        "    }\n",
        "    // wait for all threads in the block to finish\n",
        "    __syncthreads();\n",
        "\n",
        "    // each thread must update more than 1 location\n",
        "    if (PRIVATE > BLOCK_SIZE) {\n",
        "        for (int j=tx; j<PRIVATE; j+=BLOCK_SIZE) {\n",
        "            if (j < PRIVATE) {\n",
        "                atomicAdd(&(bins[j]), hist[j]);\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    // use the first `PRIVATE` threads to update final histogram\n",
        "    else {\n",
        "        if (tx < PRIVATE) {\n",
        "            atomicAdd(&(bins[tx]), hist[tx]);\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "// saturateGPU caps the bin frequencies to a maximum value of 127\n",
        "__global__ void saturateGPU(unsigned int* bins, unsigned int numBins) {\n",
        "    // global thread coordinates\n",
        "    int i = (blockIdx.x * blockDim.x) + threadIdx.x;\n",
        "\n",
        "    if (i < numBins) {\n",
        "        if (bins[i] > MAX_VAL) {\n",
        "            bins[i] = MAX_VAL;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(void) {\n",
        "    // timers\n",
        "    double timer1, timer2, timer3, timer4;\n",
        "\n",
        "    // data params\n",
        "    int inputLength;\n",
        "    unsigned int* hostBins_CPU;\n",
        "    unsigned int* hostInput; unsigned int* hostBins;\n",
        "    unsigned int* deviceInput; unsigned int* deviceBins;\n",
        "\n",
        "    // ask the user to enter the length of the input vector\n",
        "    printf(\"Please enter the length of the input array\\n\");\n",
        "    scanf(\"%d\", &inputLength);\n",
        "\n",
        "    // determine\n",
        "    size_t histoSize = NUM_BINS * sizeof(unsigned int);\n",
        "    size_t inSize = inputLength * sizeof(unsigned int);\n",
        "\n",
        "    // allocate host memory\n",
        "    hostInput = (unsigned int*)malloc(inSize);\n",
        "    hostBins = (unsigned int*)malloc(histoSize);\n",
        "    hostBins_CPU = (unsigned int*)malloc(histoSize);\n",
        "\n",
        "    // randomly initialize input array\n",
        "    srand(clock());\n",
        "    for (int i=0; i<inputLength; i++) {\n",
        "        hostInput[i] = int((float)rand()*(NUM_BINS-1)/float(RAND_MAX));\n",
        "    }\n",
        "    // for (int i=0; i<inputLength; i++) {\n",
        "    //     printf(\"%d, \", hostInput[i]);\n",
        "    // }\n",
        "    // printf(\"\\n\");\n",
        "\n",
        "    // allocate device memory\n",
        "    cudaMalloc((void**)&deviceInput, inSize);\n",
        "    cudaMalloc((void**)&deviceBins, histoSize);\n",
        "    cudaMemset(deviceBins, 0, histoSize);\n",
        "\n",
        "\n",
        "    // host2device transfer\n",
        "    timer1 = cpuSecond();\n",
        "    cudaMemcpy(deviceInput, hostInput, inSize, cudaMemcpyHostToDevice);\n",
        "    cudaDeviceSynchronize();\n",
        "    double d_t1 = cpuSecond() - timer1;\n",
        "    printf(\"Time to copy the input array from the host to the device is: %f sec.\\n\", d_t1);\n",
        "\n",
        "    // kernel launch\n",
        "    dim3 threadPerBlock(BLOCK_SIZE, 1, 1);\n",
        "    dim3 blockPerGrid(ceil(inputLength/(float)BLOCK_SIZE), 1, 1);\n",
        "    timer2 = cpuSecond();\n",
        "    histogramGPU<<<blockPerGrid, threadPerBlock>>>(deviceInput, deviceBins, inputLength);\n",
        "    cudaDeviceSynchronize();\n",
        "    double d_t2 = cpuSecond() - timer2;\n",
        "    printf(\"Implemented CUDA code for basic histogram calculation ran in: %f secs.\\n\", d_t2);\n",
        "\n",
        "    // saturate the bins\n",
        "    threadPerBlock.x = BLOCK_SIZE;\n",
        "    blockPerGrid.x = ceil(NUM_BINS/(float)BLOCK_SIZE);\n",
        "    timer3 = cpuSecond();\n",
        "    saturateGPU<<<blockPerGrid, threadPerBlock>>>(deviceBins, NUM_BINS);\n",
        "    cudaDeviceSynchronize();\n",
        "    double d_t3 = cpuSecond() - timer3;\n",
        "    printf(\"Implemented CUDA code for output saturation ran in: %f secs.\\n\", d_t3);\n",
        "\n",
        "    // device2host transfer\n",
        "    timer4 = cpuSecond();\n",
        "    cudaMemcpy(hostBins, deviceBins, histoSize, cudaMemcpyDeviceToHost);\n",
        "    double d_t4 = cpuSecond() - timer4;\n",
        "    printf(\"Time to copy the resulting Histogram from the device to the host is: %f secs.\\n\", d_t4);\n",
        "\n",
        "    // initialize CPU histogram array to 0\n",
        "    for (int i=0; i<NUM_BINS; i++) {\n",
        "        hostBins_CPU[i] = 0;\n",
        "    }\n",
        "\n",
        "    // run the CPU version\n",
        "    clock_t begin = clock();\n",
        "    histogramCPU(hostInput, hostBins_CPU, inputLength);\n",
        "    clock_t end = clock();\n",
        "\n",
        "    // printf(\"CPU: \\n\");\n",
        "    // for (int i=0; i<NUM_BINS; i++) {\n",
        "    //     printf(\"%d, \", hostBins_CPU[i]);\n",
        "    // }\n",
        "    // printf(\"\\n\");\n",
        "    // printf(\"GPU: \\n\");\n",
        "    // for (int i=0; i<NUM_BINS; i++) {\n",
        "    //     printf(\"%d, \", hostBins[i]);\n",
        "    // }\n",
        "\n",
        "    // calculate total time for CPU and GPU\n",
        "    double time_spent = (double)(end - begin) / CLOCKS_PER_SEC*1000;\n",
        "    double total_device_time = d_t1 + d_t2 + d_t3 + d_t4;\n",
        "    float speedup = (float)time_spent / total_device_time;\n",
        "    printf(\"Total CPU code ran in: %f msecs.\\n\", time_spent);\n",
        "    printf(\"Total GPU code ran in: %f secs.\\n\", total_device_time);\n",
        "    printf(\"GPU Speedup: %f\\n\", speedup);\n",
        "\n",
        "\n",
        "  \n",
        "    // release resources\n",
        "    free(hostBins); free(hostBins_CPU); free(hostInput);\n",
        "    cudaFree(deviceInput); cudaFree(deviceBins);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hl1YMiAZNp0z",
        "outputId": "ec59de98-1adf-4ea8-852d-f1968248b2ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting histo.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc histo.cu -o histo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2C48vIULVTWr",
        "outputId": "df1b06ea-838d-4f61-db82-4eba6dded6f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In file included from \u001b[01m\u001b[Khisto.cu:10:0\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/device_functions.h:54:2:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K#warning \"device_functions.h is an internal header file and must not be used directly.  This file will be removed in a future CUDA release.  Please use cuda_runtime_api.h or cuda_runtime.h instead.\" [\u001b[01;35m\u001b[K-Wcpp\u001b[m\u001b[K]\n",
            " #\u001b[01;35m\u001b[Kwarning\u001b[m\u001b[K \"device_functions.h is an internal header file and must not be used directly.  This file will be removed in a future CUDA release.  Please use cuda_runtime_api.h or cuda_runtime.h instead.\"\n",
            "  \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[Khisto.cu:10:0\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/device_functions.h:54:2:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K#warning \"device_functions.h is an internal header file and must not be used directly.  This file will be removed in a future CUDA release.  Please use cuda_runtime_api.h or cuda_runtime.h instead.\" [\u001b[01;35m\u001b[K-Wcpp\u001b[m\u001b[K]\n",
            " #\u001b[01;35m\u001b[Kwarning\u001b[m\u001b[K \"device_functions.h is an internal header file and must not be used directly.  This file will be removed in a future CUDA release.  Please use cuda_runtime_api.h or cuda_runtime.h instead.\"\n",
            "  \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/cuda-11/bin/nv-nsight-cu-cli --metrics \tsm__warps_active.avg.pct_of_peak_sustained_active histo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZkwys6jVWFt",
        "outputId": "201ce046-7a8c-466f-b563-751f6c74d166"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter the length of the input array\n",
            "10\n",
            "==PROF== Connected to process 234 (/content/histo)\n",
            "Time to copy the input array from the host to the device is: 0.000027 sec.\n",
            "==PROF== Profiling \"histogramGPU\" - 1: 0%....50%....100% - 1 pass\n",
            "Implemented CUDA code for basic histogram calculation ran in: 0.374919 secs.\n",
            "==PROF== Profiling \"saturateGPU\" - 2: 0%....50%....100% - 1 pass\n",
            "Implemented CUDA code for output saturation ran in: 0.030095 secs.\n",
            "Time to copy the resulting Histogram from the device to the host is: 0.000051 secs.\n",
            "Total CPU code ran in: 0.001000 msecs.\n",
            "Total GPU code ran in: 0.405092 secs.\n",
            "GPU Speedup: 0.002469\n",
            "==PROF== Disconnected from process 234\n",
            "[234] histo@127.0.0.1\n",
            "  histogramGPU(unsigned int*, unsigned int*, unsigned int), 2022-Dec-16 22:19:02, Context 1, Stream 7\n",
            "    Section: Command line profiler metrics\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    sm__warps_active.avg.pct_of_peak_sustained_active                                    %                          48.19\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "\n",
            "  saturateGPU(unsigned int*, unsigned int), 2022-Dec-16 22:19:03, Context 1, Stream 7\n",
            "    Section: Command line profiler metrics\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    sm__warps_active.avg.pct_of_peak_sustained_active                                    %                          48.39\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/cuda-11/bin/nv-nsight-cu-cli histo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovDUH9QABTeV",
        "outputId": "66ede60e-ce8c-4e61-c26f-7d12ba9cb576"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter the length of the input array\n",
            "10\n",
            "==PROF== Connected to process 273 (/content/histo)\n",
            "Time to copy the input array from the host to the device is: 0.000025 sec.\n",
            "==PROF== Profiling \"histogramGPU\" - 1: 0%....50%....100% - 8 passes\n",
            "Implemented CUDA code for basic histogram calculation ran in: 0.649337 secs.\n",
            "==PROF== Profiling \"saturateGPU\" - 2: 0%....50%....100% - 8 passes\n",
            "Implemented CUDA code for output saturation ran in: 0.409598 secs.\n",
            "Time to copy the resulting Histogram from the device to the host is: 0.000059 secs.\n",
            "Total CPU code ran in: 0.001000 msecs.\n",
            "Total GPU code ran in: 1.059019 secs.\n",
            "GPU Speedup: 0.000944\n",
            "==PROF== Disconnected from process 273\n",
            "[273] histo@127.0.0.1\n",
            "  histogramGPU(unsigned int*, unsigned int*, unsigned int), 2022-Dec-16 22:19:56, Context 1, Stream 7\n",
            "    Section: GPU Speed Of Light\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    DRAM Frequency                                                           cycle/nsecond                           4.86\n",
            "    SM Frequency                                                             cycle/usecond                         569.18\n",
            "    Elapsed Cycles                                                                   cycle                          3,480\n",
            "    Memory [%]                                                                           %                           1.26\n",
            "    SOL DRAM                                                                             %                           1.26\n",
            "    Duration                                                                       usecond                           6.11\n",
            "    SOL L1/TEX Cache                                                                     %                          41.09\n",
            "    SOL L2 Cache                                                                         %                           0.60\n",
            "    SM Active Cycles                                                                 cycle                          56.70\n",
            "    SM [%]                                                                               %                           0.67\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Size                                                                                                        512\n",
            "    Function Cache Configuration                                                                  cudaFuncCachePreferNone\n",
            "    Grid Size                                                                                                           1\n",
            "    Registers Per Thread                                                   register/thread                             16\n",
            "    Shared Memory Configuration Size                                                 Kbyte                          32.77\n",
            "    Driver Shared Memory Per Block                                              byte/block                              0\n",
            "    Dynamic Shared Memory Per Block                                             byte/block                              0\n",
            "    Static Shared Memory Per Block                                             Kbyte/block                          16.38\n",
            "    Threads                                                                         thread                            512\n",
            "    Waves Per SM                                                                                                     0.01\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources.            \n",
            "\n",
            "    Section: Occupancy\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Limit SM                                                                   block                             16\n",
            "    Block Limit Registers                                                            block                              8\n",
            "    Block Limit Shared Mem                                                           block                              4\n",
            "    Block Limit Warps                                                                block                              2\n",
            "    Theoretical Active Warps per SM                                                   warp                             32\n",
            "    Theoretical Occupancy                                                                %                            100\n",
            "    Achieved Occupancy                                                                   %                          47.67\n",
            "    Achieved Active Warps Per SM                                                      warp                          15.26\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "\n",
            "  saturateGPU(unsigned int*, unsigned int), 2022-Dec-16 22:19:57, Context 1, Stream 7\n",
            "    Section: GPU Speed Of Light\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    DRAM Frequency                                                           cycle/nsecond                           4.83\n",
            "    SM Frequency                                                             cycle/usecond                         564.66\n",
            "    Elapsed Cycles                                                                   cycle                          2,097\n",
            "    Memory [%]                                                                           %                           1.76\n",
            "    SOL DRAM                                                                             %                           1.76\n",
            "    Duration                                                                       usecond                           3.71\n",
            "    SOL L1/TEX Cache                                                                     %                           6.82\n",
            "    SOL L2 Cache                                                                         %                           1.15\n",
            "    SM Active Cycles                                                                 cycle                         187.80\n",
            "    SM [%]                                                                               %                           0.61\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Size                                                                                                        512\n",
            "    Function Cache Configuration                                                                  cudaFuncCachePreferNone\n",
            "    Grid Size                                                                                                           8\n",
            "    Registers Per Thread                                                   register/thread                             16\n",
            "    Shared Memory Configuration Size                                                 Kbyte                          32.77\n",
            "    Driver Shared Memory Per Block                                              byte/block                              0\n",
            "    Dynamic Shared Memory Per Block                                             byte/block                              0\n",
            "    Static Shared Memory Per Block                                              byte/block                              0\n",
            "    Threads                                                                         thread                          4,096\n",
            "    Waves Per SM                                                                                                     0.10\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources.            \n",
            "\n",
            "    Section: Occupancy\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Limit SM                                                                   block                             16\n",
            "    Block Limit Registers                                                            block                              8\n",
            "    Block Limit Shared Mem                                                           block                             16\n",
            "    Block Limit Warps                                                                block                              2\n",
            "    Theoretical Active Warps per SM                                                   warp                             32\n",
            "    Theoretical Occupancy                                                                %                            100\n",
            "    Achieved Occupancy                                                                   %                          47.88\n",
            "    Achieved Active Warps Per SM                                                      warp                          15.32\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "data = []\n",
        "binsData = np.arange(0,4096)\n",
        "\n",
        "with open(\"output\", 'r') as file:\n",
        "  for line in file:\n",
        "    data.append(int(line))\n",
        "\n",
        "ig, ax = plt.subplots(figsize =(10, 7))\n",
        "ax.hist(data)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "8Nl0MM_Xl-cU",
        "outputId": "319a215e-3caf-4306-ae20-4fccbcfa3991"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}